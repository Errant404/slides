[
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "This site is based on Quarto websites.\nTo learn more about Quarto websites visit https://quarto.org/docs/websites."
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Home",
    "section": "",
    "text": "Here’s a collection of the slides that accompanied my talk.\nYou can view by clicking on “List” in the upper left corner."
  },
  {
    "objectID": "2025-11-13-gsoc25-clad/index.html#background",
    "href": "2025-11-13-gsoc25-clad/index.html#background",
    "title": "Errant's Slides",
    "section": "Background",
    "text": "Background\n\n\n\n\n\nWhat is Automatic Differentiation (AD)?\nExact derivatives by transforming programs; no finite-difference error; supports forward and reverse.\nWhat is OpenMP?\nPragmas for shared-memory parallelism (parallel, for, reduction, etc.).\nWhat is Clad?\nSource-transformation AD library for C++ implemented as a plugin for Clang\n\n\n\n\nLet’s start with some background.\nFirst, what is Automatic Differentiation (AD)? Automatic Differentiation is a method of obtaining exact derivatives through program transformation. Unlike symbolic or numerical differentiation, it has no rounding errors or instability, and it supports both forward mode and reverse mode differentiation.\nNext, what is OpenMP? OpenMP is a widely used parallel programming interface for C, C++, and Fortran. It enables shared-memory parallelism through #pragma omp directives such as parallel, for, and reduction.\nFinally, what is Clad? Clad is a source-transformation automatic differentiation plugin for Clang. It works directly on the Clang Abstract Syntax Tree (AST) to generate derivative code. In other words, it performs “compile-time automatic differentiation,” producing exact source-level transformations for C++ functions.\nThe goal of this project, as illustrated on the right, is to apply automatic differentiation to functions annotated with OpenMP directives — so that the generated derivative code is also parallelized, achieving acceleration through parallel computation."
  },
  {
    "objectID": "2025-11-13-gsoc25-clad/index.html#case-study",
    "href": "2025-11-13-gsoc25-clad/index.html#case-study",
    "title": "Errant's Slides",
    "section": "Case Study",
    "text": "Case Study\nStart with a representative example: compute the gradient of a simple parallel loop.\ndouble parallel_sum_of_squares(const double* x, int n) {\n    double total = 0.;\n    #pragma omp parallel for reduction(+: total)\n    for (int i = 0; i &lt; n; i++) {\n        total += x[i] * x[i];\n    }\n    return total;\n}\n\nLet’s start with a simple OpenMP example. This function parallel_sum_of_squares computes the sum of squares of all elements in an array. It parallelizes the loop using #pragma omp parallel for reduction(+: total)."
  },
  {
    "objectID": "2025-11-13-gsoc25-clad/index.html#case-study-1",
    "href": "2025-11-13-gsoc25-clad/index.html#case-study-1",
    "title": "Errant's Slides",
    "section": "Case Study",
    "text": "Case Study\nWhat might the reverse-mode gradient look like conceptually?\nvoid parallel_sum_of_squares_grad(const double *x, int n, double *_d_x, int *_d_n) {\n    double _d_total = 0.;\n    double total = 0.;\n    #pragma omp parallel for reduction(+: total)\n    for (int i = 0; i &lt; n; i++) {\n        total += x[i] * x[i];\n    }\n    _d_total += 1;\n    #pragma omp parallel private(total) firstprivate(_d_total)\n    for (int i = n - 1; i &gt;= 0; i -= 1) {\n        double _r_d0 = _d_total;\n        _d_x[i] += _r_d0 * x[i];\n        _d_x[i] += x[i] * _r_d0;\n    }\n}\n\nNow, we’d like to obtain its reverse-mode gradient. Conceptually, we want to automatically generate an equivalent reverse-mode function. The slide shows a conceptual example, and you can see that:\n\nWe first perform the same forward loop as in the original function.\nThen, we initialize the derivative of the result variable total (i.e., _d_total) to 1.\nFinally, in the reverse loop, we accumulate the gradients for each element x[i].\n\nIs that correct? Almost — but there are still some subtleties and implementation details to handle, which we’ll look into next. The basic idea, however, is already quite close."
  },
  {
    "objectID": "2025-11-13-gsoc25-clad/index.html#technical-challenges",
    "href": "2025-11-13-gsoc25-clad/index.html#technical-challenges",
    "title": "Errant's Slides",
    "section": "Technical Challenges",
    "text": "Technical Challenges\n\n\n\nClang AST specifics for OpenMP (building AST nodes, capture variables, clause handling)\nVariable scoping across OpenMP regions and clauses\nThread-safe storage for tapes/intermediates\nDeterministic schedule reversal for the reverse pass\n\n\n\nThere are several core challenges here:\n\nComplexity of OpenMP nodes in the Clang AST We need to correctly build and capture variables within parallel regions and handle different OpenMP clauses.\nVariable scoping Each thread in an OpenMP region has its own private variables, and we must ensure these are properly matched between the forward and reverse passes.\nThread-safe storage of intermediates Reverse-mode AD requires saving intermediate values (so-called “tapes”), and these must be thread-safe.\nDeterministic schedule reversal We must guarantee that during the reverse pass, each thread’s iteration chunk is identical to that in the forward pass.\n\nThe diagram above shows the data dependencies between intermediate results. Each thread maintains its own stack. The key is that the same thread must perform the corresponding forward and reverse iterations — in reverse order — to ensure that data is popped correctly from its local stack."
  },
  {
    "objectID": "2025-11-13-gsoc25-clad/index.html#implementation-overview",
    "href": "2025-11-13-gsoc25-clad/index.html#implementation-overview",
    "title": "Errant's Slides",
    "section": "Implementation Overview",
    "text": "Implementation Overview\n\n\nThe theory is based on the paper from Tapenade, which provides detailed proofs and demonstrates the implementation of OpenMP automatic differentiation on the Fortran platform using Tapenade.\nSince OpenMP constructs are transformed into AST nodes during the Clang AST phase, we can override the corresponding OpenMP-related AST nodes Visit methods—just as we do with other Visit methods—to build new function ASTs.\n\n\n\n\n\n\nFor implementation, we took inspiration from the Tapenade team’s paper (linked in the slides). Tapenade has already implemented OpenMP automatic differentiation for Fortran and provides solid theoretical foundations.\nOur approach in Clad is similar. Since OpenMP constructs are represented as specific AST nodes in Clang, we only need to extend the corresponding Visit methods in the AST visitor to produce differentiated versions. In other words, we reuse Clang’s AST mechanisms so that Clad can correctly identify and reconstruct OpenMP statements.\nThe slide also links to a Compiler Explorer example, where we can see the actual OpenMP AST structure — including a node type called CapturedStmt, which represents captured variables in a parallel region."
  },
  {
    "objectID": "2025-11-13-gsoc25-clad/index.html#forward-mode-1491",
    "href": "2025-11-13-gsoc25-clad/index.html#forward-mode-1491",
    "title": "Errant's Slides",
    "section": "Forward Mode (#1491)",
    "text": "Forward Mode (#1491)\n\n\nSince the execution order of the forward-mode derivatives is consistent with that of the original code, the scopes of the differentiated variables can be directly inherited from the original variables, and the parallel structure of the entire program can also be reused.\nFor the previous example, the actually generated code looks like this:\n\n\n\n// auto d_fn_arr = clad::differentiate(parallel_sum_of_squares, \"x[1]\");\n// d_fn_arr.dump();\ndouble parallel_sum_of_squares_darg0_1(const double *x, int n) {\n    int _d_n = 0;\n    double _d_total = 0.;\n    double total = 0.;\n    #pragma omp parallel for reduction(+: _d_total,total)\n        for (int i = 0; i &lt; n; i++) {\n            _d_total += (i == 1) * x[i] + x[i] * (i == 1);\n            total += x[i] * x[i];\n        }\n    return _d_total;\n}\n\n\nI already presented the Forward Mode implementation during the midterm evaluation. Forward mode is relatively straightforward because its execution order matches the original program. Therefore, we can directly reuse the same parallel structure, with identical variable scopes.\nIn this example, the generated forward-mode code is shown below. You can see that:\n\nBoth total and _d_total appear in the same parallel region.\nWe simply add an additional reduction clause reduction(+: _d_total, total) to accumulate the derivatives.\n\nThe structure is almost identical to the original function, making it very intuitive."
  },
  {
    "objectID": "2025-11-13-gsoc25-clad/index.html#reverse-mode-clang-ast",
    "href": "2025-11-13-gsoc25-clad/index.html#reverse-mode-clang-ast",
    "title": "Errant's Slides",
    "section": "Reverse Mode — Clang AST",
    "text": "Reverse Mode — Clang AST\n\n\nThe previous VisitForStmt method was too invasive for omp for; we implement a dedicated DifferentiateCanonicalLoop for OpenMP loops.\n\n\n\n// auto fn_grad = clad::gradient(sum_of_squares);\n// fn_grad.dump();\nvoid sum_of_squares_grad(const double *x, int n, double *_d_x, int *_d_n) {\n    int _d_i = 0;\n    int i = 0;\n    double _d_total = 0.;\n    double total = 0.;\n    unsigned long _t0 = 0;\n    for (i = 0; i &lt; n; i++) {\n        _t0++;\n        total += x[i] * x[i];\n    }\n    _d_total += 1;\n    for (; _t0; _t0--) {\n        i--;\n        {\n            double _r_d0 = _d_total;\n            _d_x[i] += _r_d0 * x[i];\n            _d_x[i] += x[i] * _r_d0;\n        }\n    }\n}\n\n\nReverse mode, however, is much more complex. We can’t just modify the existing VisitForStmt, because its loop counter is embedded within the loop and the generated ForStmt cannot be correctly divided among threads.\nTherefore, we introduced a new helper function called DifferentiateCanonicalLoop, which specifically handles OpenMP loops and returns a canonical ForStmt suitable for OpenMP task partitioning and scheduling."
  },
  {
    "objectID": "2025-11-13-gsoc25-clad/index.html#reverse-mode-clang-ast-1",
    "href": "2025-11-13-gsoc25-clad/index.html#reverse-mode-clang-ast-1",
    "title": "Errant's Slides",
    "section": "Reverse Mode — Clang AST",
    "text": "Reverse Mode — Clang AST\n\n\nIt should be noted that, for Clang’s OpenMP implementation, we need to capture the related variables when building DeclRefExpr (which corresponds to the CaptureStmt’s subnodes in the Clang AST).\nSo we also need visit twice to generate two OpenMP regions and their capture lists: one for the forward pass, one for the reverse pass.\n\n\n\nCLAD_COMPAT_CLANG19_SemaOpenMP(m_Sema).ActOnOpenMPRegionStart(OMPD_parallel, nullptr);\nStmtDiff BodyDiff;\n{\n    Sema::CompoundScopeRAII CompoundScope(m_Sema);\n    if (isOpenMPLoopDirective(D-&gt;getDirectiveKind())) {\n        const auto* FS = cast&lt;ForStmt&gt;(CS);\n        BodyDiff = DifferentiateCanonicalLoop(FS);\n    } else {\n        BodyDiff = Visit(CS);\n    }\n}\nStmt* Forward = CLAD_COMPAT_CLANG19_SemaOpenMP(m_Sema)\n                    .ActOnOpenMPRegionEnd(BodyDiff.getStmt(), OrigClauses)\n                    .get();\n\nCLAD_COMPAT_CLANG19_SemaOpenMP(m_Sema).ActOnOpenMPRegionStart(OMPD_parallel, nullptr);\n// Visit twice, but use only the result of the first visit, for capture variables only.\n{\n    Sema::CompoundScopeRAII CompoundScope(m_Sema);\n    Stmts temp;\n    m_Globals.swap(temp);\n    if (isOpenMPLoopDirective(D-&gt;getDirectiveKind())) {\n        const auto* FS = cast&lt;ForStmt&gt;(CS);\n        DifferentiateCanonicalLoop(FS);\n    } else {\n        Visit(CS);\n    }\n    m_Globals.swap(temp);\n}\nStmt* Reverse =\n    CLAD_COMPAT_CLANG19_SemaOpenMP(m_Sema)\n        .ActOnOpenMPRegionEnd(BodyDiff.getStmt_dx(), DiffClauses)\n        .get();\n\n\nIn Clang’s OpenMP implementation, each parallel region uses a CapturedStmt to capture external variables. Thus, when constructing the AST for the reverse pass, we must traverse twice: once to generate the forward region and its capture list, and again to generate the reverse region and its capture list.\nThis ensures that each OpenMP region has the correct variable context."
  },
  {
    "objectID": "2025-11-13-gsoc25-clad/index.html#reverse-mode-scope-conversion",
    "href": "2025-11-13-gsoc25-clad/index.html#reverse-mode-scope-conversion",
    "title": "Errant's Slides",
    "section": "Reverse Mode — Scope conversion",
    "text": "Reverse Mode — Scope conversion\n\n\nThis diagram shows how scope conversion works. In short, Clad maps variables from each scope in the forward pass to the corresponding scope in the reverse pass. This mapping is especially important in OpenMP, where we distinguish between private, shared, and firstprivate variables.\nThe example shows a loop with 9 iterations (represented as boxes with iteration numbers), parallelized with two threads and a hypothetical scheduling scheme assigning two iteration chunks per thread. In the multithreaded view, colors represent thread assignment, arrows indicate data flow, and the markers denote variable creation, zero initialization, or accumulation operations. The top half shows the behavior of the original and tangent (forward) modes, while the bottom half shows the adjoint (reverse) behavior.\nThe paper provides detailed theoretical proofs of correctness, which I won’t cover here."
  },
  {
    "objectID": "2025-11-13-gsoc25-clad/index.html#reverse-mode-intermediates-and-schedule-replay",
    "href": "2025-11-13-gsoc25-clad/index.html#reverse-mode-intermediates-and-schedule-replay",
    "title": "Errant's Slides",
    "section": "Reverse Mode — Intermediates and schedule replay",
    "text": "Reverse Mode — Intermediates and schedule replay\n\n\nIntermediates require tapes; the tape itself must become private to each thread; so we make these threadprivate.\nFor data consistency, we must ensure that the parallel scheduling of the backward pass strictly reproduces the thread–task allocation of the forward pass. To achieve this, we reverse the schedule while preserving chunk boundaries by a small runtime helper:\n\nForward pass: each thread computes its static chunk from thread ID and executes normally.\nReverse pass: the same helper returns identical chunks, then we iterate the chunk in reverse order.\n\n\n\n\nvoid clad::GetStaticSchedule(int lo, int hi, int stride, int* threadlo, int* threadhi)\n\n\nAnother key issue is schedule replay. Since OpenMP loop scheduling is often static, we can compute each thread’s iteration range using a small runtime helper: clad::GetStaticSchedule.\nDuring the reverse pass, we call the same helper again to obtain the same chunk boundaries and then iterate them in reverse order. This ensures that each thread precisely “replays” its original forward work during the backward pass, maintaining consistency at the thread level."
  },
  {
    "objectID": "2025-11-13-gsoc25-clad/index.html#reverse-mode-1641",
    "href": "2025-11-13-gsoc25-clad/index.html#reverse-mode-1641",
    "title": "Errant's Slides",
    "section": "Reverse Mode (#1641)",
    "text": "Reverse Mode (#1641)\nThe resulting generated code:\n\n// auto fn_grad = clad::gradient(parallel_sum_of_squares);\n// fn_grad.dump();\nvoid parallel_sum_of_squares_grad(const double *x, int n, double *_d_x, int *_d_n) {\n    double _d_total = 0.;\n    double total = 0.;\n    #pragma omp parallel reduction(+: total)\n        {\n            int _t_chunklo0 = 0;\n            int _t_chunkhi0 = 0;\n            clad::GetStaticSchedule(0, n - 1, 1, &_t_chunklo0, &_t_chunkhi0);\n            for (int i = _t_chunklo0; i &lt;= _t_chunkhi0; i += 1) {\n                total += x[i] * x[i];\n            }\n        }\n    _d_total += 1;\n    #pragma omp parallel private(total) firstprivate(_d_total)\n        {\n            int _t_chunklo1 = 0;\n            int _t_chunkhi1 = 0;\n            clad::GetStaticSchedule(0, n - 1, 1, &_t_chunklo1, &_t_chunkhi1);\n            for (int i = _t_chunkhi1; i &gt;= _t_chunklo1; i -= 1) {\n                {\n                    double _r_d0 = _d_total;\n                    _d_x[i] += _r_d0 * x[i];\n                    _d_x[i] += x[i] * _r_d0;\n                }\n            }\n        }\n}\n\n\nFinally, this slide shows the complete generated reverse-mode code. As you can see:\n\nThe forward pass and backward pass are placed in two separate parallel regions.\nThe GetStaticSchedule helper ensures identical chunk boundaries.\nEach thread iterates its chunk in reverse during the backward pass.\nGradients for each element x[i] are accumulated into _d_x[i].\n\nThe structure fully matches the theoretical expectation and works correctly under multithreaded execution."
  },
  {
    "objectID": "2025-11-13-gsoc25-clad/index.html#future-work",
    "href": "2025-11-13-gsoc25-clad/index.html#future-work",
    "title": "Errant's Slides",
    "section": "Future Work",
    "text": "Future Work\n\nSupport dynamic schedules (record executed chunks at runtime and replay).\nSupport more OpenMP clauses and directives (atomic, simd, etc.).\nWe can even try to enable OpenMP offloading to target accelerators (e.g., GPUs).\n\n\nFor future improvements:\n\nSupport dynamic scheduling For dynamically scheduled loops, we’ll need to record the executed chunks at runtime and replay them during the backward pass.\nSupport additional OpenMP clauses and directives Such as atomic, simd, and potentially OpenMP target offloading, enabling automatic differentiation on accelerators like GPUs."
  },
  {
    "objectID": "list.html",
    "href": "list.html",
    "title": "List",
    "section": "",
    "text": "Order By\n      Default\n      \n        Date - Oldest\n      \n      \n        Date - Newest\n      \n      \n        Title\n      \n    \n  \n\n\n\n\n\n\nDate\n\n\n\nTitle\n\n\n\n\n\n\n\n\nNov 13, 2025\n\n\nEnable Automatic Differentiation of OpenMP Programs with Clad\n\n\n\n\n\n\nJul 16, 2025\n\n\nCapriccio: Scalable Threads for Internet Services\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "2025-07-16-capriccio/index.html#main-contribution",
    "href": "2025-07-16-capriccio/index.html#main-contribution",
    "title": "Errant's Slides",
    "section": "Main Contribution",
    "text": "Main Contribution\n\nScalable user-level thread package\n\nAlternative to\n\nEvent-based models\nKernel-thread models\n\n\nScalability to 100,000 threads\nEfficient for Internet Servers\n\n\n这篇论文主要是讲了一个叫做 Capriccio 线程包，经过论文作者的精心设计，它是 Scalable 的，最大可以扩展到 10 万线程，在后面实验中，作者证明了它对 web 服务器（比如说 Apache）是高效的。 这篇论文还有一个很有意思的历史北京，它是对当时正在进行的所有基于事件的工作的反驳，这是 OSTEP 书上说的，我们后面也会介绍这段背景"
  },
  {
    "objectID": "2025-07-16-capriccio/index.html#key-features",
    "href": "2025-07-16-capriccio/index.html#key-features",
    "title": "Errant's Slides",
    "section": "Key Features",
    "text": "Key Features\n\nScalability with user-level threads\n\nCooperative scheduling\nAsynchronous disk I/O\nEfficient thread operations - O(1)\n\nLinked stack management\nResource-aware scheduling\n\n\n这篇论文中，作者的核心方法可以分为三块。第一是通过用户态线程实现 Scalability，第二是实现一种高效的链式栈管理系统，第三是借助阻塞图引入了资源感知调度"
  },
  {
    "objectID": "2025-07-16-capriccio/index.html#outline",
    "href": "2025-07-16-capriccio/index.html#outline",
    "title": "Errant's Slides",
    "section": "Outline",
    "text": "Outline\n\nRelated Work and “Debate”\nCapriccio Scalability with user-level threads\nLinked Stack Management\nResource-Aware Scheduling\n\n\n这是我今天讲 paper 的大纲"
  },
  {
    "objectID": "2025-07-16-capriccio/index.html#debate-event-based-side",
    "href": "2025-07-16-capriccio/index.html#debate-event-based-side",
    "title": "Errant's Slides",
    "section": "Debate – event-based side",
    "text": "Debate – event-based side\n\nEvent-based arguments by Ousterhout (Why threads are bad?, 1996)\n\nEvents are more efficient (no context switching and locking overheads with threads)\nThreads - hard to program (deadlocks, synchronization)\nPoor thread support (portability, debugging)\n\nMany event-based implementation (Harvest, Flash, SEDA)\n\n\n学术界关于高并发场景下最佳编程模型的争论由来已久，其中线程与事件驱动模型之争尤为突出。Ousterhout 曾系统阐述事件驱动模型的诸多优势。 近年来，在可扩展服务器架构的研究中，事件驱动模型也得到广泛推崇，典型应用包括 Flash、Harvest 等互联网服务器，以及 SEDA、Ninja 等服务器基础架构。"
  },
  {
    "objectID": "2025-07-16-capriccio/index.html#debate-other-arguments",
    "href": "2025-07-16-capriccio/index.html#debate-other-arguments",
    "title": "Errant's Slides",
    "section": "Debate – other arguments",
    "text": "Debate – other arguments\n\n\nNeutral argument by Lauer and Needham (On the duality of OS system structures, 1978)\n\nAny system constructed according to a model can have a direct counterpart in the other model\n\nPro-thread arguments by Behren, Condit, Brewer (Why events are bad?, 2003)\n\nGreater code readability\nNo “stack-ripping”\nSlow thread performance - implementation artifact\nHigh performance servers more sensitive to scheduling\n\n\n\n\n但事实真的如此吗？ 一种模型的优势往往天然成为另一种模型的劣势，关于哪种模型更优的争论由来已久。然而，H.C. Lauer 和 R. M. Needham 在 1978 年发表的实证研究中指出，这两种模型实际上互为对偶关系——基于任一模型构建的系统，都能在另一模型中找到对应实现。值得注意的是，论文中提出的模型具有理想化特征，现实中并不存在完全属于某一类别的系统。在实际系统底层，往往是两种模型的混合体，只有上层架构才体现特定模型特征。 依照 Lauer 和 Needham 提出的二元性论证传统，我们此前曾指出，事件表现出的所谓优势其实只是线程实现不佳的假象。因此，我们认为以往支持事件的论点更应理解为：针对特定应用的优化需求以及对高效线程运行时的追求。这两点正是推动 Capriccio 研发的核心动因。值得一提的是，Capriccio 调度器采用的阻塞图设计灵感直接来源于 SEDA 架构中的阶段划分和显式队列机制。 而相较于基于事件的模型，基于线程的模型的好处在于它更易理解，并且没有 stack-ripping（每当程序可能发生阻塞调用时，开发者就不得不拆分调用堆栈，转而通过回调函数创建闭包来触发后续操作）。 本文除了给出了一种基于用户态线程的模型之外，我认为最有价值的一点是它启发了我们对编程模型其本身的思考。"
  },
  {
    "objectID": "2025-07-16-capriccio/index.html#capriccio",
    "href": "2025-07-16-capriccio/index.html#capriccio",
    "title": "Errant's Slides",
    "section": "Capriccio",
    "text": "Capriccio\n\n\nPhilosophy\n\nThread model is useful\nImprove implementation to remove barriers to scalability\n\nTechniques\n\nUser-level threads\nLinked stack management\nResource aware scheduling\n\nTools\n\nCompiler-analysis\nRun-time monitoring\n\n\n\n\n于是基于前面的观点，作者们设计了 Capriccio，它兼容目前的线程的接口，通过一系列技术手段提升应用在多线程情况下的性能，并且无需修改原程序的大部分代码。"
  },
  {
    "objectID": "2025-07-16-capriccio/index.html#why-user-level-threads",
    "href": "2025-07-16-capriccio/index.html#why-user-level-threads",
    "title": "Errant's Slides",
    "section": "Why user-level threads?",
    "text": "Why user-level threads?\n\n\nDecoupling from the OS/kernel\n\nOS independence\nKernel variation\nAddress application-specific needs 1\n\nCooperative threading – more efficient synchronization\nLess “kernel crossing”\nMore efficient memory management\n\n\n\n那么为什么会用用户态线程呢？主要是有这些好处 它可以将应用和操作系统平台解耦，从而隐藏操作系统间的差异与变化，适应操作系统的快速发展。解耦使得上层的线程包能够灵活地利用内核的最新特性（如文中提到的新型异步I/O机制）来提升性能和可伸缩性，同时又不必被绑定在某个特定或发展缓慢的内核上。 内核线程无法针对特定应用定制调度算法。而用户级线程则不受此限制，其调度器可以与应用程序同步开发，实现更贴合应用需求的调度策略。 此外由于用户态线程采用的是协程调度，因此线程之间的同步非常容易；而且相较于系统级线程，线程切换不会陷入内核态，其本身也相较于内核态线程也要更为轻量\n\ne.g. take advantage of a new asynchronous I/O interface and the user-level thread scheduler can be built along with the application."
  },
  {
    "objectID": "2025-07-16-capriccio/index.html#implementation",
    "href": "2025-07-16-capriccio/index.html#implementation",
    "title": "Errant's Slides",
    "section": "Implementation",
    "text": "Implementation\n\nNon-blocking wrappers for blocking I/O\nAsynchronous disk I/O where possible 1\nScheduling loop resembling an event-driven application\nCheap synchronization\nEfficient O(1) thread operations\n\n\n为了实现可伸缩的基本操作，Capriccio通过覆盖GNU libc存根函数在库级别拦截阻塞I/O调用。这些调用在内部被非阻塞等效项替代，允许线程包在线程启动I/O操作时保持控制。系统使用Edgar Toernig的协程库进行极快的上下文切换，当线程自愿让出时，并采用高效的算法，确保大多数线程管理函数的O(1)最坏情况运行时间。\n\nIf these mechanisms are not available, Capriccio falls back on the standard Unix poll() call for pollable descriptors and a pool of kernel threads for disk I/O."
  },
  {
    "objectID": "2025-07-16-capriccio/index.html#benchmarks",
    "href": "2025-07-16-capriccio/index.html#benchmarks",
    "title": "Errant's Slides",
    "section": "Benchmarks",
    "text": "Benchmarks\n\n\n\n(left) Capriccio scales to 100,000 threads\n(right) Network I/O throughput with Capriccio only has 10% overhead over epoll\n\n\n\n图1展示了在生产者-消费者模型下，不同线程包（Capriccio, LinuxThreads, NPTL）的调度和同步效率。 图2通过一个管道测试来模拟网络I/O的可扩展性，比较了 Capriccio 与 LinuxThreads、NPTL 以及两种I/O模型（poll 和 epoll）的性能。"
  },
  {
    "objectID": "2025-07-16-capriccio/index.html#benchmarks-1",
    "href": "2025-07-16-capriccio/index.html#benchmarks-1",
    "title": "Errant's Slides",
    "section": "Benchmarks",
    "text": "Benchmarks\n\n\n\nWith asynchronous I/O disk performance is comparable in Capriccio vs. other thread packages"
  },
  {
    "objectID": "2025-07-16-capriccio/index.html#disadvantages-of-user-level-threads",
    "href": "2025-07-16-capriccio/index.html#disadvantages-of-user-level-threads",
    "title": "Errant's Slides",
    "section": "Disadvantages of user-level threads",
    "text": "Disadvantages of user-level threads\n\nNon-blocking wrappers of blocking I/O increase kernel crossings\nExtra overhead for function calls\nDifficult to integrate with multiple processor scheduling"
  },
  {
    "objectID": "2025-07-16-capriccio/index.html#linked-stacks",
    "href": "2025-07-16-capriccio/index.html#linked-stacks",
    "title": "Errant's Slides",
    "section": "Linked Stacks",
    "text": "Linked Stacks\n \n\n\nProblem: Conservative stack allocations per thread are unsuitable for programs with many threads.\nSolution: Dynamic stack allocation with linked chunks alleviates VM pressure and improves paging behavior.\nMethod: Compile-time analysis and checkpoint injection into the code."
  },
  {
    "objectID": "2025-07-16-capriccio/index.html#linked-stacks-algorithm",
    "href": "2025-07-16-capriccio/index.html#linked-stacks-algorithm",
    "title": "Errant's Slides",
    "section": "Linked Stacks: Algorithm",
    "text": "Linked Stacks: Algorithm\n\n\n\n\n\n\n\n\n\n\nEach node is a call site annotated with the maximum stack space for that call.\nCheckpoints must be inserted at each recursive frame and well-spaced call sites.\nCheckpoints determine whether to allocate a new stack chunk.\n\n\n\n\nIn the figure, the MaxPath parameter is 8."
  },
  {
    "objectID": "2025-07-16-capriccio/index.html#scheduling-blocking-graph",
    "href": "2025-07-16-capriccio/index.html#scheduling-blocking-graph",
    "title": "Errant's Slides",
    "section": "Scheduling: Blocking Graph",
    "text": "Scheduling: Blocking Graph\n\n\n\nLessons from event systems\n\nBreak app into stages\nSchedule based on stage priorities \n\nCapriccio does this for threads\n\nDeduce stage with stack traces at blocking points\nPrioritize based on runtime information"
  },
  {
    "objectID": "2025-07-16-capriccio/index.html#resource-aware-scheduling",
    "href": "2025-07-16-capriccio/index.html#resource-aware-scheduling",
    "title": "Errant's Slides",
    "section": "Resource-Aware Scheduling",
    "text": "Resource-Aware Scheduling\n\n\nTrack resources used along BG edges\n\nMemory, file descriptors, CPU\nPredict future from the past\n\nAlgorithm\n\nIncrease use when underutilized\nDecrease use near saturation\n\nAdvantages\n\nOperate near the knee w/o thrashing\nAutomatic admission control"
  },
  {
    "objectID": "2025-07-16-capriccio/index.html#pitfalls",
    "href": "2025-07-16-capriccio/index.html#pitfalls",
    "title": "Errant's Slides",
    "section": "Pitfalls",
    "text": "Pitfalls\n\nMaximum capacity of a particular resource is difficult to determine\nThrashing is not easily detectable.\nNon-yielding threads lead to unfairness and starvation in cooperative scheduling.\nBlocking graphs are expensive to maintain (for Apache 2.0.44 stack trace overhead is 8% of execution time)."
  },
  {
    "objectID": "2025-07-16-capriccio/index.html#web-server-performance",
    "href": "2025-07-16-capriccio/index.html#web-server-performance",
    "title": "Errant's Slides",
    "section": "Web Server Performance",
    "text": "Web Server Performance\n\n\n\nApache 2.0.44 on a 4x500 MHz Pentium server has 15% higher throughput with Capriccio."
  },
  {
    "objectID": "2025-07-16-capriccio/index.html#conclusion",
    "href": "2025-07-16-capriccio/index.html#conclusion",
    "title": "Errant's Slides",
    "section": "Conclusion",
    "text": "Conclusion\n\n\nCapriccio demonstrates a user-level thread package that achieves\n\nHigh scalability\nEfficient stack management\nScheduling based on resource usage\n\nDrawbacks \n\nHigh overhead in stack tracing\nLack of sufficient multi-processor support"
  },
  {
    "objectID": "2025-07-16-capriccio/index.html#future-work",
    "href": "2025-07-16-capriccio/index.html#future-work",
    "title": "Errant's Slides",
    "section": "Future Work",
    "text": "Future Work\n\nExtending Capriccio to work with multiple processors\nReducing the kernel crossings with batching asynchronous network I/O\nDisambiguate function pointers in stack allocation\nImproving resource-aware scheduling\n\nTracking variance in resource usage\nBetter detection of thrashing\n\n\n\nTODO: 继任者：Goroutine"
  }
]